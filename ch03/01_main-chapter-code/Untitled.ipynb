{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2f0f48-189e-4796-9ddb-3851eb8226b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46cf3919-af57-49e2-a310-82629d023d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa55ac17-7285-4b99-94b8-82dbcf5856b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ron/Documents/github/LLMs-from-scratch-main/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc4fcc1-e70e-4c56-ab35-897847c35c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ron/Documents/github/LLMs-from-scratch-main/ch03/01_main-chapter-code\n"
     ]
    }
   ],
   "source": [
    "!PWD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501de971-7af2-4a6e-8913-98e8143f7ac1",
   "metadata": {},
   "source": [
    "## A simple self-attention Mechanism without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad488e6-6c9b-403a-b1ae-d15654aa9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979e4fe7-580e-4e64-85e2-f1742f379b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1]\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c150ec8d-a2e0-4815-bca4-062e6f2f49c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e2a338-40e8-43d2-ae46-1dd106bd8fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be24878-0a93-417f-bb6e-cf1639a932f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f28b879-0e1b-4ad0-952e-23b2642c79a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "input_query = inputs[1]\n",
    "attn_score_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_score_2[i] =  torch.dot(x_i, input_query)\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ea12f4-1144-471b-ba46-f95cac093eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_2_temp = attn_score_2 / attn_score_2.sum(axis=-1)\n",
    "attn_weight_2_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1cf9ca-3bb7-4428-8eb8-1183d73d9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_2_temp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b19a4707-de67-40ae-9d76-062e95b82ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sofmax_naive(x):\n",
    "    return torch.exp(x)/ torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0260ac24-3b28-4f63-8777-3f07eec43e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = sofmax_naive(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab197aca-22a1-479d-ab3d-b7f54575f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_score_2, dim=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1716698-3452-467d-9759-3957fb3ce8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca0f5141-aa8f-4716-b874-87fbbd537a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b0e9747-99f9-43bf-bdda-627bf1cb3fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ea5a8-697a-4a48-9ea0-1297cf403fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b40c276-101e-4503-ac59-60401f55f7fd",
   "metadata": {},
   "source": [
    " ## Simple Self-attn mechanism witrhout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f29c87d-e273-4c52-bf3e-005be8137e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6e49b-a2e9-4187-9a2e-7c73d0ddbcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afc97bcc-a563-4329-9a21-274885f03624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_score = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_score[i,j] =  torch.dot(x_i, x_j)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "995ffb77-2f79-40ea-ad11-d832dcc3e0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e854cb79-cea1-46f4-a37b-435409b9c468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score = inputs @ inputs.T\n",
    "attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c25f04a-ccd7-4618-9050-fc6e470239a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = torch.softmax(attn_score, dim=1)\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23d653cb-0965-4225-bcb1-68e6b3e2b2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c61e82e8-36b8-4120-afee-d501e4030b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = torch.softmax(attn_score, dim=1)\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c292fe7e-f108-4090-ab79-da9dff91f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vecs = attn_weight @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "760c5a75-bf82-43c0-968b-1bc6dbdd9ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score = inputs @ inputs.T\n",
    "attn_weight = torch.softmax(attn_score, dim=1)\n",
    "all_context_vecs = attn_weight @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a3255-cacc-46e4-978f-12a22b394375",
   "metadata": {},
   "source": [
    "# 3.4 Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376908e-385a-4dc0-9600-4f54b1fe187d",
   "metadata": {},
   "source": [
    "## 3.4.1 Computing the attention weights step by step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3f9bc25-1ad2-4012-b9ef-32fd18650eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cdb37ed-8688-4231-a692-f6736e36481a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae688ff5-f887-400c-a80b-aa9779d92f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e80633d2-cd13-4663-8188-ebea1e9cf9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6358c862-7c45-4bf4-84ec-862955d49c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab1cdce3-34b0-43ac-a33b-313be40d029e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1366, 0.1025],\n",
       "        [0.1841, 0.7264],\n",
       "        [0.3153, 0.6871]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e15193bb-f520-4a27-82c1-9cc14b3ee0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0756, 0.1966],\n",
       "        [0.3164, 0.4017],\n",
       "        [0.1186, 0.8274]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb091a2-21ce-4822-8a96-fdfbbc4ba209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "293eb649-c1cb-4e35-869e-d2e8dd527843",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key\n",
    "value = inputs @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c56cafcb-0e27-45e8-85e5-4fd821769235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "668fed4a-e26b-4a04-90b1-e8722f4a8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = torch.dot(query_2, keys_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff046963-d7c8-4a98-a3ce-07698797919d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c94ace70-030b-410a-93bd-8a627f0c3e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score_2 = query_2 @ keys.T\n",
    "attn_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0da0f51e-828f-4a4b-b13d-b8985b3340a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k**0.5,  dim = -1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "511b96cf-1023-49dd-a2e3-f769b0bba2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "414be982-23e7-4ffb-95a8-b22939395833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ value\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df9183-d11f-4957-a58a-c484589b09e9",
   "metadata": {},
   "source": [
    " ## Implementing Compact self-attention with trainable weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e23c1794-4cc8-4564-b985-8c336de5bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        d_k = keys.shape[1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbea5676-ac55-48f8-817b-885381539441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v11(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores  = queries @ keys.T\n",
    "        k_dim = keys.shape[1]\n",
    "        attention_weights =  torch.softmax(attention_scores / k_dim ** 0.5, axis =-1)\n",
    "        context_vec = attention_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31e09fae-d942-442f-a9c4-878dd9ad245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7fdf6718-536c-4e8e-985f-84c42b8bca3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f46c58c-5a94-4858-b7c9-c700c3f7a4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=3, bias=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Linear(2, 3)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f716504b-08ca-4c05-bd03-fe8705096110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1668,  0.2270],\n",
       "        [ 0.5000,  0.1317],\n",
       "        [ 0.1934,  0.6825]], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3511b0a-7758-4fb0-b2e2-a1aad029c27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3189,  0.2240, -0.3146], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5da24a3-0cf9-455a-afb2-f1fd1be65842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        d_k = keys.shape[1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a519f6b4-4f86-4211-b9be-20d78f4e24b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v2(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb19f48-c458-4c3b-87a9-653cce6ca26e",
   "metadata": {},
   "source": [
    "# 3.5 Hiding Future Words with Causal Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4872e7f-7c43-480c-abbf-d5ac37e046ae",
   "metadata": {},
   "source": [
    "## 3.5.1 Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ea0fd65-5933-4739-b7a8-0e8117e406b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "values = sa_v2.W_value(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb916fa1-bf87-4cd7-9a39-69897e978337",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = queries @ keys.T\n",
    "d_k = keys.shape[1]\n",
    "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad5c2a49-5d8b-4a51-8535-fdee20760ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
       "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
       "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a46f6f7-9b10-4fd5-9bde-eae754319a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee002d37-875f-42fe-bcbe-5eaca380fea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mased_simple = attn_weights * mask_simple\n",
    "mased_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd62000e-78ca-4d0f-98bc-3c38a2652062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = mased_simple.sum(dim=-1, keepdim=True)\n",
    "mased_simple_norm = mased_simple/row_sums\n",
    "print(mased_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01c89b2c-c8c0-4b11-9fe9-c1e5aa6f05f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f80ad474-ab00-4169-91b5-21c8055887b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7ea07f0-9637-43af-ade6-627740ec8b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0603a0-939e-4a4b-9a9b-6f2fda4412f4",
   "metadata": {},
   "source": [
    " ## 3.5.2 Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c3a3fe8-5660-46eb-85aa-5fa3e717f912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11a925bf0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb751862-c891-4c5b-957a-d3a6b4141b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Dropout(0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9293fd5-6efe-4f81-a97f-34584d5c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.ones(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c9d07ce-9a81-4564-89fc-778a2627fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8149b0d-4329-4e07-8484-aa14d5eb3230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be9c5cf4-68fe-4a9a-9335-dcfdad860704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f33b28b2-fdfe-4494-8412-2a0259727f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(layer(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "956624ff-14d2-42f0-b83a-e40eb5ab074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.5\n",
    "1/ (1 - dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc728d42-e959-476f-9795-3446a34139f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.1034, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.3968, 0.3775, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3327, 0.3331, 0.0000, 0.0000, 0.3058]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ea522-33ce-4867-a782-14d4ae95aab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a226ca9-b359-43e4-af94-449f29011a2d",
   "metadata": {},
   "source": [
    "## 3.5.3 Implementing a compact causal self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f6489-b4e5-4c5e-a002-ecf0e84388f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "052766cb-943b-41aa-8d89-397cb5930e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        d_k = keys.shape[1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fef22d1c-d75d-4c4d-bfda-e39d181d1013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ad73f07-46b3-4322-b0ac-a824b99db9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22b9041f-e24f-49af-be03-cbf0b420c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "context_length = batch.shape[1]\n",
    "dropout = 0.0\n",
    "d_in = batch.shape[2]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "590a1d55-9f1f-45cf-8994-d30bf8e6bc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0872,  0.0286],\n",
       "         [-0.0996,  0.0511],\n",
       "         [-0.1004,  0.0645],\n",
       "         [-0.0984,  0.0489],\n",
       "         [-0.0512,  0.1102],\n",
       "         [-0.0761,  0.0682]],\n",
       "\n",
       "        [[-0.0872,  0.0286],\n",
       "         [-0.0996,  0.0511],\n",
       "         [-0.1004,  0.0645],\n",
       "         [-0.0984,  0.0489],\n",
       "         [-0.0512,  0.1102],\n",
       "         [-0.0761,  0.0682]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = CausalAttention(d_in, d_out, context_length, dropout)\n",
    "ca(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1adfd-fe01-4464-b97c-14fbe347c4c3",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e7d30-85d7-4c27-b849-7d087e7777ac",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e09fdd5-cf08-4cce-a9dd-4fb30e497c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads=2, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim =-1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "32e163d2-68ce-4591-b86d-1e659dee16cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5856,  0.0087,  0.5840,  0.3158],\n",
       "         [-0.6284, -0.0607,  0.6161,  0.3779],\n",
       "         [-0.5664, -0.0832,  0.5468,  0.3557],\n",
       "         [-0.5511, -0.0978,  0.5317,  0.3416],\n",
       "         [-0.5290, -0.1073,  0.5072,  0.3465]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5856,  0.0087,  0.5840,  0.3158],\n",
       "         [-0.6284, -0.0607,  0.6161,  0.3779],\n",
       "         [-0.5664, -0.0832,  0.5468,  0.3557],\n",
       "         [-0.5511, -0.0978,  0.5317,  0.3416],\n",
       "         [-0.5290, -0.1073,  0.5072,  0.3465]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "dropout = 0.0\n",
    "d_in = batch.shape[-1]\n",
    "d_out = 2\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n",
    "mha(batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d1017-8d67-4f28-80cb-c17e03fd3794",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "643866a3-b1c4-4c16-9cff-402e4bb482ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]],\n",
      "\n",
      "        [[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forwar\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 4\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e6bebe83-7c4c-449d-97cd-2206a495aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]],\n",
      "\n",
      "        [[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forwar\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2,3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 4\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9d08d-873f-4915-91f1-e3ec50dc4c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
